{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186f7940",
   "metadata": {
    "id": "186f7940"
   },
   "outputs": [],
   "source": [
    "# Importing\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "import h5py as h5\n",
    "import cv2\n",
    "from osgeo import gdal, osr\n",
    "import tarfile\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba827cb7",
   "metadata": {},
   "source": [
    "## Necessary functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93bfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coordinates from TIF image in its coordinate system\n",
    "def get_coords_from_tif(path):\n",
    "    # Get the existing coordinate system\n",
    "    ds = gdal.Open(path)\n",
    "    \n",
    "    width = ds.RasterXSize\n",
    "    height = ds.RasterYSize\n",
    "    gt = ds.GetGeoTransform()\n",
    "    dx = gt[1]\n",
    "    dy = -gt[5]\n",
    "    \n",
    "    minx = gt[0]\n",
    "    miny = gt[3] + width*gt[4] + height*gt[5] \n",
    "    \n",
    "    #maxx = gt[0] + width*gt[1] + height*gt[2]\n",
    "    #maxy = gt[3] \n",
    "    centerx = gt[0] + width/2*gt[1] + height/2*gt[2]\n",
    "    centery = gt[3] + width/2*gt[4] + height/2*gt[5]\n",
    "    center = (centerx, centery)\n",
    "    sizes = (width*dx, height*dy)\n",
    "    return center, sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b920977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert TIF coordinates into lattitude and longitude\n",
    "def transorm_point_to_latlong(x,y, path):\n",
    "    ds = gdal.Open(path)\n",
    "    \n",
    "    # projective coordinate system\n",
    "    old_cs_config = \"\"\"\n",
    "    PROJCS[\"WGS 84 / UTM zone 39N\",\n",
    "        GEOGCS[\"WGS 84\",\n",
    "            DATUM[\"WGS_1984\",\n",
    "                SPHEROID[\"WGS 84\",6378137,298.257223563,\n",
    "                    AUTHORITY[\"EPSG\",\"7030\"]],\n",
    "                AUTHORITY[\"EPSG\",\"6326\"]],\n",
    "            PRIMEM[\"Greenwich\",0,\n",
    "                AUTHORITY[\"EPSG\",\"8901\"]],\n",
    "            UNIT[\"degree\",0.0174532925199433,\n",
    "                AUTHORITY[\"EPSG\",\"9122\"]],\n",
    "            AUTHORITY[\"EPSG\",\"4326\"]],\n",
    "        PROJECTION[\"Transverse_Mercator\"],\n",
    "        PARAMETER[\"latitude_of_origin\",0],\n",
    "        PARAMETER[\"central_meridian\",51],\n",
    "        PARAMETER[\"scale_factor\",0.9996],\n",
    "        PARAMETER[\"false_easting\",500000],\n",
    "        PARAMETER[\"false_northing\",0],\n",
    "        UNIT[\"metre\",1,\n",
    "            AUTHORITY[\"EPSG\",\"9001\"]],\n",
    "        AXIS[\"Easting\",EAST],\n",
    "        AXIS[\"Northing\",NORTH],\n",
    "        AUTHORITY[\"EPSG\",\"32639\"]]\n",
    "    \"\"\"\n",
    "    old_cs = osr.SpatialReference()\n",
    "    old_cs .ImportFromWkt(old_cs_config)\n",
    "    \n",
    "    # create the new coordinate system\n",
    "    wgs84_wkt = \"\"\"\n",
    "    GEOGCS[\"WGS 84\",\n",
    "        DATUM[\"WGS_1984\",\n",
    "            SPHEROID[\"WGS 84\",6378137,298.257223563,\n",
    "                AUTHORITY[\"EPSG\",\"7030\"]],\n",
    "            AUTHORITY[\"EPSG\",\"6326\"]],\n",
    "        PRIMEM[\"Greenwich\",0,\n",
    "            AUTHORITY[\"EPSG\",\"8901\"]],\n",
    "        UNIT[\"degree\",0.01745329251994328,\n",
    "            AUTHORITY[\"EPSG\",\"9122\"]],\n",
    "        AUTHORITY[\"EPSG\",\"4326\"]]\"\"\"\n",
    "    new_cs = osr.SpatialReference()\n",
    "    new_cs .ImportFromWkt(wgs84_wkt)\n",
    "    \n",
    "    \n",
    "    # create a transform object to convert between coordinate systems\n",
    "    transform = osr.CoordinateTransformation(old_cs,new_cs) \n",
    "    #get the coordinates in lat long\n",
    "    latlong = transform.TransformPoint(x,y)\n",
    "    return latlong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22327bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open and extract relevant scene's images in cache directory\n",
    "def extract_scene_to_cache(path_to_data, s):\n",
    "    path = path_to_data+'/'+s\n",
    "    with tarfile.open(path) as tar:\n",
    "        files = glob.glob('../DATA/cache/*')\n",
    "        for f in files:\n",
    "            os.chmod(f, 0o777)\n",
    "            os.remove(f)\n",
    "        #data_list = os.listdir(tar)\n",
    "        for i,m in enumerate(tar.getmembers()):\n",
    "            if (m.name.endswith('.TIF') and 'SR_B' in m.name):\n",
    "                tar.extract(m, '../DATA/cache/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f4d754",
   "metadata": {
    "id": "97f4d754"
   },
   "outputs": [],
   "source": [
    "# calculate the angle of image rotation (to get rid of black triangles)\n",
    "def get_angle(image_npy):\n",
    "    x1_max = 0\n",
    "    x2_max = 0\n",
    "    for b in range(7):\n",
    "        y1 = image_npy.shape[1]-1000\n",
    "        x1 = np.nonzero(image_npy[:,y1,b])[0][0]\n",
    "        if x1>x1_max:\n",
    "            x1_max = x1\n",
    "        y2 = image_npy.shape[1]-5000\n",
    "        x2 = np.nonzero(image_npy[:,y2,b])[0][0]\n",
    "        if x2>x2_max:\n",
    "            x2_max = x2\n",
    "    tan = (y1-y2)/(x1_max-x2_max)\n",
    "    phi_rad = np.pi/2 - np.arctan(tan)\n",
    "    return phi_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "980ed5d9",
   "metadata": {
    "id": "980ed5d9"
   },
   "outputs": [],
   "source": [
    "#crop image on some delta pixels from edges after the rotation\n",
    "def crop_black(new_npy, delta = 200):\n",
    "    x_min = np.nonzero(new_npy[:,new_npy.shape[1]-5000,0])[0][0]+delta\n",
    "    x_max = np.nonzero(new_npy[:,new_npy.shape[1]-5000,0])[0][-1]-delta\n",
    "    y_min = np.nonzero(new_npy[5000,:,0])[0][0]+delta\n",
    "    y_max = np.nonzero(new_npy[5000,:,0])[0][-1]-delta\n",
    "    return new_npy[x_min:x_max,y_min:y_max,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9cfc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class that works with scene (TIF images) as a numpy array\n",
    "class numpy_image():\n",
    "    def __init__(self,s_name,path_to_data):\n",
    "        \n",
    "        #name of current scene\n",
    "        self.s = s_name\n",
    "        \n",
    "        #extracting images of scene from .tar to cache\n",
    "        extract_scene_to_cache(path_to_data, self.s)\n",
    "        \n",
    "        #path to extracted scene\n",
    "        self.path = '../DATA/cache/'\n",
    "        \n",
    "        #collecting cached tifs into one numpy array\n",
    "        name_tifs = sorted(os.listdir(self.path))\n",
    "        scene = []\n",
    "        for nt in name_tifs:\n",
    "            path_to_tif = self.path + nt  \n",
    "            #z print(path_to_tif)\n",
    "            band = cv2.imread(path_to_tif)[:,:,0]\n",
    "            scene.append(band)\n",
    "        self.npy = np.array(scene).transpose(1,2,0)\n",
    "        \n",
    "        #calculating angle to rotate\n",
    "        self.phi_rad = get_angle(self.npy)\n",
    "        self.phi = self.phi_rad/np.pi * 180\n",
    "        \n",
    "        #rotation of image and cropping black boundaries\n",
    "        self.npy = rotate(self.npy, self.phi)\n",
    "        self.npy = crop_black(self.npy)\n",
    "        \n",
    "        #calculating eventual transformation in terms of affine trasfrom\n",
    "        self.center, self.sizes = get_coords_from_tif(path_to_tif)\n",
    "        w = self.sizes[0] #size along x1 axes\n",
    "        h = self.sizes[1] #size along y1 axes\n",
    "        self.a = (-h*np.cos(self.phi_rad)+w*np.sin(self.phi_rad))/(np.sin(self.phi_rad)**2-np.cos(self.phi_rad)**2) #size along x2 axes\n",
    "        self.b = (h*np.sin(self.phi_rad)-w*np.cos(self.phi_rad))/(np.sin(self.phi_rad)**2-np.cos(self.phi_rad)**2) #size along y2 axes\n",
    "        \n",
    "        (x2_pixels, y2_pixels) = self.npy[:,:,0].shape\n",
    "        (self.dx2, self.dy2) = (self.a/x2_pixels, self.b/y2_pixels)\n",
    "    \n",
    "    #get actual coordinated of a pixel\n",
    "    def get_geo_coords(self, x2,y2):\n",
    "        x1 = self.center[0]+x2*np.cos(self.phi_rad)+y2*np.sin(self.phi_rad)\n",
    "        y1 = self.center[1]-x2*np.sin(self.phi_rad)+y2*np.cos(self.phi_rad)\n",
    "        return x1,y1\n",
    "    \n",
    "    #getting batch of small images from the initial one, concat with coordinates info\n",
    "    def make_cropped_batch(self, size = 512):\n",
    "        Shape = self.npy.shape\n",
    "        y_num = Shape[0]//size\n",
    "        x_num = Shape[1]//size\n",
    "        img_list = []\n",
    "\n",
    "        x2_up_left = -self.b/2\n",
    "        y2_up_left = self.a/2\n",
    "        coords_list = []\n",
    "\n",
    "        for i in range(y_num):\n",
    "            for j in range(x_num):\n",
    "                img = self.npy[i*size:(i+1)*size,j*size:(j+1)*size,:]\n",
    "                img_list.append(img)\n",
    "                x2 = x2_up_left + self.dx2*size*j\n",
    "                y2 = y2_up_left - self.dy2*size*i\n",
    "                x_u_l, y_u_l = self.get_geo_coords(x2,y2)\n",
    "                coords_list.append([x_u_l,y_u_l])\n",
    "        return np.array(img_list), np.array(coords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc655ce4",
   "metadata": {},
   "source": [
    "## Rotate big scene, cut to set of small images, get coordinates for each, make HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9de20f76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 539649,
     "status": "ok",
     "timestamp": 1681736069995,
     "user": {
      "displayName": "Альберт Мацейко",
      "userId": "13845313783107003992"
     },
     "user_tz": -360
    },
    "id": "9de20f76",
    "outputId": "1a75e79b-c407-4f6e-ecbe-3b0540343365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset for the 1'st scene is created\n",
      "2'th scene is added\n",
      "3'th scene is added\n",
      "4'th scene is added\n",
      "5'th scene is added\n",
      "6'th scene is added\n"
     ]
    }
   ],
   "source": [
    "### Path to raw data\n",
    "path_to_data = '../DATA/data_raw/'\n",
    "scenes = sorted(os.listdir(path_to_data))\n",
    "\n",
    "### Creating dir for h5 files if necessary\n",
    "try:\n",
    "    os.mkdir('../DATA/h5_files/')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "### Set desireble size of small images\n",
    "size = 256\n",
    "\n",
    "### Set True if you want to make a small dataset\n",
    "SMALL_DATASET = True\n",
    "if SMALL_DATASET == True:\n",
    "    # 1/10 of all scenes will be added to dataset\n",
    "    scenes = scenes[0:len(scenes)//10] \n",
    "    h5_name = 'LC08_L2SP_02_T1_' + str(size)+'_SMALL.h5'\n",
    "else:\n",
    "    h5_name = 'LC08_L2SP_02_T1_' + str(size)+'.h5'\n",
    "    \n",
    "### Path where to create h5 files\n",
    "path_to_h5 = '../DATA/h5_files/' + h5_name\n",
    "\n",
    "\n",
    "\n",
    "### Creates an unnormed h5 dataset\n",
    "with h5.File(path_to_h5, 'w') as f:\n",
    "    ### Iterting over .tar files\n",
    "    for n,s in enumerate(scenes):\n",
    "        \n",
    "        ### Converting scene to numpy image\n",
    "        image = numpy_image(s, path_to_data)\n",
    "            \n",
    "        ### Making batch of small images SIZExSIZExCHANNELS and their coords\n",
    "        batch, coords = image.make_cropped_batch(size)\n",
    "        \n",
    "        ### Adding the batch and coords to h5 dataset\n",
    "        if n == 0:\n",
    "            f.create_dataset('all/data_raw', data=batch, maxshape=(None,size,size,7))\n",
    "            f.create_dataset('all/geo_coords', data=coords, maxshape=(None,2))\n",
    "            print(\"Initial dataset for the 1'st scene is created\")\n",
    "        else:\n",
    "            f['all/data_raw'].resize((f['all/data_raw'].shape[0] + batch.shape[0]), axis = 0)\n",
    "            f['all/data_raw'][-batch.shape[0]:] = batch\n",
    "            f['all/geo_coords'].resize((f['all/geo_coords'].shape[0] + coords.shape[0]), axis = 0)\n",
    "            f['all/geo_coords'][-coords.shape[0]:] = coords\n",
    "            print(str(n+1)+\"'th scene is added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51bb917",
   "metadata": {},
   "source": [
    "## Normalizing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4752df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3014\n",
      "Step number is: 0\n",
      "Step number is: 100\n",
      "Step number is: 200\n",
      "Step number is: 300\n",
      "Step number is: 400\n",
      "Step number is: 500\n",
      "Step number is: 600\n",
      "Step number is: 700\n",
      "Step number is: 800\n",
      "Step number is: 900\n",
      "Step number is: 1000\n",
      "Step number is: 1100\n",
      "Step number is: 1200\n",
      "Step number is: 1300\n",
      "Step number is: 1400\n",
      "Step number is: 1500\n",
      "Step number is: 1600\n",
      "Step number is: 1700\n",
      "Step number is: 1800\n",
      "Step number is: 1900\n",
      "Step number is: 2000\n",
      "Step number is: 2100\n",
      "Step number is: 2200\n",
      "Step number is: 2300\n",
      "Step number is: 2400\n",
      "Step number is: 2500\n",
      "Step number is: 2600\n",
      "Step number is: 2700\n",
      "Step number is: 2800\n",
      "Step number is: 2900\n",
      "Step number is: 0\n",
      "Step number is: 100\n",
      "Step number is: 200\n",
      "Step number is: 300\n",
      "Step number is: 400\n",
      "Step number is: 500\n",
      "Step number is: 600\n",
      "Step number is: 700\n",
      "Step number is: 800\n",
      "Step number is: 900\n",
      "Step number is: 1000\n",
      "Step number is: 1100\n",
      "Step number is: 1200\n",
      "Step number is: 1300\n",
      "Step number is: 1400\n",
      "Step number is: 1500\n",
      "Step number is: 1600\n",
      "Step number is: 1700\n",
      "Step number is: 1800\n",
      "Step number is: 1900\n",
      "Step number is: 2000\n",
      "Step number is: 2100\n",
      "Step number is: 2200\n",
      "Step number is: 2300\n",
      "Step number is: 2400\n",
      "Step number is: 2500\n",
      "Step number is: 2600\n",
      "Step number is: 2700\n",
      "Step number is: 2800\n",
      "Step number is: 2900\n"
     ]
    }
   ],
   "source": [
    "### Calculating MEAN and VARIANCE for each channel ###\n",
    "\n",
    "# Path to h5 file that needed to be normalized\n",
    "path_to_h5_to_norm = '../DATA/h5_files/' + h5_name\n",
    "with h5.File(path_to_h5_to_norm, 'r+') as f:\n",
    "    # Initialize sum and sum of square variances for each channel\n",
    "    SUM = 0\n",
    "    SUM_SQ = 0\n",
    "    \n",
    "    # Number of small images in h5 file\n",
    "    N = f['all/data_raw'].shape[0]\n",
    "    print(N)\n",
    "    \n",
    "    # Size of every image in h5 file\n",
    "    size = f['all/data_raw'].shape[1]\n",
    "    \n",
    "    # Set step to iterate over the file\n",
    "    dN = 50\n",
    "    \n",
    "    # Calculating SUM and MEAN for each channel\n",
    "    for i in range(0,N-dN,dN):\n",
    "        SUM += np.sum(f['all/data_raw'][i:i+dN], axis = (0,1,2))\n",
    "        if i%100 == 0: print('Step number is:', i)\n",
    "        i_last = i+dN\n",
    "    SUM += np.sum(f['all/data_raw'][i_last:], axis = (0,1,2))\n",
    "    MEAN = np.array(SUM, dtype = 'float64')/(N*size**2)\n",
    "    \n",
    "    # Calculating SUM_SQ and dispersion for each channel\n",
    "    for i in range(0,N-dN,dN):\n",
    "        SUM_SQ += np.sum((f['all/data_raw'][i:i+dN]-MEAN)**2, axis = (0,1,2))\n",
    "        if i%100 == 0: print('Step number is:', i)\n",
    "        i_last = i+dN\n",
    "    SUM_SQ += np.sum((f['all/data_raw'][i_last:]-MEAN)**2, axis = (0,1,2))\n",
    "    DISP = SUM_SQ/(N*size**2)\n",
    "    SIGMA = np.sqrt(DISP)\n",
    "    \n",
    "    # Adding info about mean and dispersion in the h5 dataset\n",
    "    try:\n",
    "        del f['all/norm_params']\n",
    "    except:\n",
    "        pass\n",
    "    f.create_dataset('all/norm_params/mean_values', data=MEAN, maxshape=(7,))\n",
    "    f.create_dataset('all/norm_params/sigma_values', data=SIGMA, maxshape=(7,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2351404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number is:  0\n",
      "Batch number is:  100\n",
      "Batch number is:  200\n",
      "Batch number is:  300\n",
      "Batch number is:  400\n",
      "Batch number is:  500\n",
      "Batch number is:  600\n",
      "Batch number is:  700\n",
      "Batch number is:  800\n",
      "Batch number is:  900\n",
      "Batch number is:  1000\n",
      "Batch number is:  1100\n",
      "Batch number is:  1200\n",
      "Batch number is:  1300\n",
      "Batch number is:  1400\n",
      "Batch number is:  1500\n",
      "Batch number is:  1600\n",
      "Batch number is:  1700\n",
      "Batch number is:  1800\n",
      "Batch number is:  1900\n",
      "Batch number is:  2000\n",
      "Batch number is:  2100\n",
      "Batch number is:  2200\n",
      "Batch number is:  2300\n",
      "Batch number is:  2400\n",
      "Batch number is:  2500\n",
      "Batch number is:  2600\n",
      "Batch number is:  2700\n",
      "Batch number is:  2800\n",
      "Batch number is:  2900\n",
      "h5 file is normilized!\n",
      "Now keys of the file are: <KeysViewHDF5 ['data_norm', 'geo_coords', 'norm_params']>\n"
     ]
    }
   ],
   "source": [
    "### Replacing dataset with normilized one ###\n",
    "\n",
    "# Creating datasets with normilized data\n",
    "with h5.File(path_to_h5_to_norm, 'r+') as f:\n",
    "    try:\n",
    "        del f['all/data_norm']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Get mean and variance\n",
    "    MEAN = f['all/norm_params/mean_values']\n",
    "    SIGMA = f['all/norm_params/sigma_values']\n",
    "    \n",
    "    # Get number of small images\n",
    "    N = f['all/data_raw'].shape[0]\n",
    "    size = f['all/data_raw'].shape[1]\n",
    "    \n",
    "    # Set step to iterate over the file\n",
    "    bs = 100\n",
    "    \n",
    "    # Normalizing data iterating over file, adding to data_norm dataset\n",
    "    for i in range(0,N-bs,bs):\n",
    "        batch = np.array(f['all/data_raw'][i:i+bs], dtype = 'float64')\n",
    "        batch_norm = np.array((batch-MEAN)/SIGMA, dtype = 'float32')\n",
    "        if i == 0:\n",
    "            f.create_dataset('all/data_norm', data=batch_norm, maxshape=(None,size,size,7))\n",
    "        else:\n",
    "            f['all/data_norm'].resize((f['all/data_norm'].shape[0] + bs), axis = 0)\n",
    "            f['all/data_norm'][-bs:] = batch_norm\n",
    "        print('Batch number is: ', i)\n",
    "        i_last = i+bs\n",
    "        \n",
    "    # Normilizing remainder\n",
    "    batch = np.array(f['all/data_raw'][i_last:], dtype = 'float64')\n",
    "    batch_norm = np.array((batch-MEAN)/SIGMA, dtype = 'float32')\n",
    "    if i_last == 0:\n",
    "        f.create_dataset('all/data_norm', data=batch_norm, maxshape=(None,size,size,7))\n",
    "    else:\n",
    "        f['all/data_norm'].resize((f['all/data_norm'].shape[0] + batch_norm.shape[0]), axis = 0)\n",
    "        f['all/data_norm'][-batch_norm.shape[0]:] = batch_norm\n",
    "    print('h5 file is normilized!')\n",
    "    \n",
    "    # Delete raw dataset, leaving normalized one\n",
    "    del f['all/data_raw']\n",
    "    \n",
    "    print('Now keys of the file are:', f['all'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca47a6",
   "metadata": {},
   "source": [
    "### Now HDF5 file is created in DATA/h5_files/ catalog."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
